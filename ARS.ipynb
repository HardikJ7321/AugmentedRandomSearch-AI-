{"cells":[{"metadata":{"_uuid":"5075c0a3-d62b-431b-902e-904c36461a2c","_cell_guid":"e03e2fde-1899-4519-94d8-d93f474642f8","trusted":true},"cell_type":"code","source":"\n# importing libraries \nimport gym\nfrom gym import wrappers\nimport numpy as np # linear algebra\nimport os\nimport pybullet_envs\n\n\n\nclass Hp():                                 #hyperparameter class\n    \n    def __init__(self):\n        self.num_steps = 1000               #no of times the update takes place\n        self.episode_length = 1000\n        self.learning_rate = 0.02\n        self.num_directions = 16            #no. of small changes(+/-)\n        self.num_best_directions = 16       #direction having max reward\n        assert self.num_best_directions <= self.num_directions\n        self.noise = 0.03                   #standard deviation(sigma),taken small to have low variance\n        self.seed = 1\n        self.env_name = \"HalfCheetahBulletEnv-v0\"\n    \n\nclass Normalizer():                       #for normalizing the states\n    \n    def __init__(self,num_inputs):\n        self.mean = np.zeros(num_inputs)\n        self.n = np.zeros(num_inputs)            #counter for counting no of states  \n        self.mean_diff = np.zeros(num_inputs)    #for the numerator of variance eqn\n        self.var = np.zeros(num_inputs)\n     \n    #updates the variables each time we encounter a new state\n    def observe(self, x):      #x is the new state\n        self.n += 1.           #increases the total no. of states by one\n        last_mean = self.mean\n        self.mean += (x - self.mean) / self.n    #new mean after the new state\n        self.mean_diff += (x - last_mean)*(x - self.mean)   #new numerator for variance eqn\n        self.var = ((self.mean_diff) / self.n).clip(min = 1e-2)    #variance can never be 0 as it will be reqd in denom in normalisation\n    \n    def normalizer(self, inputs):           #takes the input as state vars\n        obs_mean = self.mean\n        obs_std = np.sqrt(self.var)\n        return (inputs - obs_mean) / obs_std       #returns the normalized state\n    \n#building the ai\n\nclass Policy():\n    \n    def __init__(self, input_size, output_size):\n        self.theta = np.zeros((output_size, input_size))\n        \n    def evaluate(self, input, delta = None, direction = None):    #delta is the small pertubation & direction is +/-\n        if direction is None:\n            return self.theta.dot(input)                      #when no pertubation is applied\n        elif direction is 'positive':\n            return (self.theta + hp.noise*delta).dot(input)   #for positive direction of pertubation\n        else:\n            return (self.theta - hp.noise*delta).dot(input)   #for opp. direction of pertubation\n        \n    def sample_deltas(self):\n        return [np.random.randn(*self.theta.shape) for _ in range(hp.num_directions)]    #(*theta.shape) is used so that both dimensions of theta are passed as size for delta\n                #will return lsit of 16 diff metrices containing random numbers as pertubations\n    \n    def update(self, rollouts, sigma_r):        #rollout refers to the combination of +ve and -ve reward generated by any pertubation metrix d\n        step = np.zeros(self.theta.shape)\n        for r_pos, r_neg, d in rollouts:\n            step += (r_pos - r_neg) * d         #method of finite differences\n        self.theta += (hp.learning_rate)/(hp.num_best_directions * sigma_r) * step    \n    \n\n\n#explore the policy in one specific direction on one full episode    \ndef explore(env, normalizer, policy, direction = None, delta = None):       #env is a pybullet object\n    state = env.reset()\n    done = False\n    num_plays = 0\n    sum_rewards = 0\n    while not done and num_plays < hp.episode_length:\n        normalizer.observe(state)\n        state = normalizer.normalize(state)\n        action = policy.evaluate(state, delta, direction)\n        state, reward, done, _ = env.step(action)\n        reward = max(min(reward, 1), -1)       #this statement takes care of the outliers.\n                                               #if reward >1, it makes it 1 and if reward <-1, it makes it -1\n        sum_rewards += reward\n        num_plays += 1\n    return sum_rewards         #returns the total reward for each episode\n\n\n#training the ai\ndef train(env, policy, normalizer, hp):\n    \n    for step in range(hp.num_steps):\n        #initialising the pertubation deltas and pos/neg rewards\n        deltas = policy.sample_deltas()\n        positive_rewards = [0] * hp.num_directions\n        negative_rewards = [0] * hp.num_directions          #rewards for pertubations in neg direction\n    \n        #getting the +ve rewards in +ve directions and -ve in -ve direction\n        for k in range(hp.num_directions):\n            positive_rewards[k] = explore(env, normalizer, policy, direction = \"positive\", delta = deltas[k])\n            negative_rewards[k] = explore(env, normalizer, policy, direction = \"negative\", delta = deltas[k])\n        \n        #concetenating both rewards into one numpy array so that to calculate sigma_r    \n        all_rewards = np.array(positive_rewards + negative_rewards)\n        sigma_r = all_rewards.std()\n        \n        #sorting rollouts by max(r_pos, r_neg) and best directions\n        scores = {k:max(r_pos, r_neg) for k,(r_pos, r_neg) in enumerate(zip(positive_rewards, negative_rewards))}\n        order = sorted(scores.keys(), key = lambda x:scores[x])[:hp.num_best_directions]\n                    #sorts the scores dict with respect to value in key-value pairs of scores dict\n        rollouts = [(positive_rewards[k], negative_rewards[k], deltas[k]) for k in order] \n        \n        #updating the policy \n        policy.update(rollouts, sigma_r) \n        \n        #reward evaluation\n        reward_evaluation = explore(env, normalizer, policy)\n        print(\"step \",step,\" reward: \",reward_evaluation)\n        \n\n        \n#running the main code\ndef mkdir(base, name):\n    path = os.path.join(base, name)\n    if not os.path.exists(path):\n        os.makedirs(path)\n    return path    \nwork_dir = mkdir('exp', 'ars')\nmonitor_dir = mkdir(work_dir, 'monitor')         #to create a dir for saving the monitor videos\n\nhp = Hp()\nnp.random.seed(hp.seed)\nenv = gym.make(hp.env_name)                      #creating a gym environment\nenv = wrappers.Monitor(env, monitor_dir, force = True)     #force = True forces to overrun all warning signs\nnum_inputs = env.observation_space.shape[0]                #number of inputs from environment\nnum_outputs = env.action_space.shape[0]                    #number of actions performed (outputs)\npolicy = Policy(num_inputs, num_outputs)\nnormalizer = Normalizer(num_inputs)\ntrain(env, policy, normalizer, hp)","execution_count":0,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}